{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bptripp/ai-course/blob/main/digit_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0CMfJI4hQ76"
      },
      "source": [
        "#Categorizing Images with Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3xs-nZyx5g3"
      },
      "source": [
        "The code below imports some packages and defines a small convolutional network class similar to the one you saw earlier. It is meant to classify images of handwritten digits from 0-9.\n",
        "\n",
        "*Run this code to get started.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3PxLa0yCgwCw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvolutionalNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvolutionalNetwork, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "    self.dropout = nn.Dropout2d(.5)\n",
        "    self.fc1 = nn.Linear(9216, 128)\n",
        "    self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = self.conv1(input)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "    x = self.dropout(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds2yfn5T3FnX"
      },
      "source": [
        "Digit recognition was the first thing convolutional networks were used for in the 1980s. The motivation was to automatically sort US mail by ZIP code. To help solve this problem, a set of 70,000 digit images was curated in the Modified National Institute of Standards and Technology (MNIST) dataset (https://en.wikipedia.org/wiki/MNIST_database).\n",
        "\n",
        "You will now load the MNIST data and train a ConvolutionalNetwork with it. MNIST is one of the most popular machine learning datasets, so PyTorch knows where it is and will download it for you. The code below will also prepare training and testing datasets and corresponding \"DataLoader\" objects that provide batches of examples from these datasets.\n",
        "\n",
        "*Run the code below to download and organize the data.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5815k83SL_3",
        "outputId": "4f743f92-fd1e-4462-e4a9-8a60b51a99ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 78382328.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 112894402.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 79007155.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20729628.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "directory = 'data' # the folder where the dataset will be saved on this server\n",
        "\n",
        "# this object will convert images into the right form for input to the network\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# these objects contain training and testing subsets of the data\n",
        "train_set = datasets.MNIST(directory, train=True, download=True, transform=transform)\n",
        "test_set = datasets.MNIST(directory, train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 1000 # the size of groups of examples that are used for gradient descent steps\n",
        "\n",
        "#these objects will provide batches of examples from the datasets\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HikKs3zNa-Qn"
      },
      "source": [
        "You will now create a function that loops through all the batches of image/label examples in a dataset, runs them through the network, and keeps track of the average loss and accuracy in the process. If this function gets an optimizer object as a parameter, it will also perform backpropagation and a variation of gradient descent to update the parameters.   \n",
        "\n",
        "*Run the code below to create a function that passes a dataset through the network.*  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(model, loader, optimizer=None):\n",
        "  \"\"\"\n",
        "  model: the deep network\n",
        "  loader: the dataloader for the dataset to be processed\n",
        "  optimizer: object that updates parameters\n",
        "  \"\"\"\n",
        "\n",
        "  # create lists of losses and accuracies for each batch in the dataset\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "\n",
        "  for inputs, labels in loader:\n",
        "    print('.', end='') # prints a dot for every batch as a simple progress bar\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(inputs) # run the inputs through the network\n",
        "    loss = nn.CrossEntropyLoss()(outputs, labels) # calculate loss\n",
        "\n",
        "    if optimizer is not None:\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward() # backpropagation\n",
        "      optimizer.step() # gradient descent\n",
        "\n",
        "    # calculate the fraction of predictions in this batch that were correct\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    fraction_correct = torch.sum(predictions == labels.data).item() / len(predictions)\n",
        "\n",
        "    # add this batch's loss and fraction-correct to the lists\n",
        "    losses.append(loss.item())\n",
        "    accuracies.append(fraction_correct)\n",
        "\n",
        "  # print the average loss and accuracy over the whole dataset\n",
        "  print('{} loss: {:.4f} accuracy: {:.4f}'.format(\n",
        "      'Validation' if optimizer is None else 'Training',\n",
        "      np.mean(losses),\n",
        "      np.mean(accuracies)))"
      ],
      "metadata": {
        "id": "slFiZTw_8cBE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO"
      ],
      "metadata": {
        "id": "oegSbJgpHX6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "pLprHsLqHZzH",
        "outputId": "ba8a79ca-677b-4f8a-9080-b39c25d03747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO"
      ],
      "metadata": {
        "id": "tjml2dfRIeaX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WCnfUumdGNq",
        "outputId": "db0935c3-cc36-471c-8c6f-b58571973cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "............................................................Training loss: 1.0323 accuracy: 0.6948\n",
            "..........Validation loss: 0.3377 accuracy: 0.9077\n",
            "Epoch 2/3\n",
            "............................................................Training loss: 0.2763 accuracy: 0.9181\n",
            "..........Validation loss: 0.2005 accuracy: 0.9405\n",
            "Epoch 3/3\n",
            "............................................................Training loss: 0.1870 accuracy: 0.9455\n",
            "..........Validation loss: 0.1346 accuracy: 0.9604\n"
          ]
        }
      ],
      "source": [
        "from torch import optim\n",
        "\n",
        "model = ConvolutionalNetwork()\n",
        "model.to(device);\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# train_model(model, optimizer, 3);\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "\n",
        "    model.train() # put model in training mode\n",
        "    process_dataset(model, train_loader, optimizer=optimizer)\n",
        "\n",
        "    model.eval() # put model in evaluation mode\n",
        "    process_dataset(model, validation_loader)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v0CMfJI4hQ76"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
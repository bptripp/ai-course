{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bptripp/ai-course/blob/main/degree_of_mixing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantifying the Mixing of Outcomes\n",
        "Decision trees split examples so that the outcomes are as uniform as possible within each group. Different decision tree algorithms minimize different measures of non-uniformity, including Gini impurity and information-theoretic entropy. For this course it is not necessary to understand exactly how these measures work. However, if you are interested, read on below for more detail on entropy.\n",
        "\n",
        "Entropy is the degree of uncertainty in a variable (e.g. the decision to walk or not). This kind of entropy is related to but distinct from thermodynamic “entropy”, which is the degree of disorder in a system. Information-theoretic entropy is highest when different outcomes are equally likely, and zero when the outcome is certain. A unit of entropy is called a “bit”. A fair coin toss that has not yet occurred has one bit of entropy, and its outcome has one bit of information.\n",
        "\n",
        "To calculate entropy:\n",
        "1.\tList the probabilities of different outcomes. For example, in a coin toss, the probabilities of heads and tails are both 0.5. We could write this fact as $P_H=P_T=0.5$.\n",
        "2.\tCalculate the base-2 logarithm of each probability, i.e. $\\log_2P$. The base-2 logarithm of a number is the exponent of 2 that produces that number. $2^{-1}=0.5$, so the $\\log_20.5=-1$. A probability of 1 (the highest possible probability) has a logarithm of 0. Lower probabilities have negative logarithms. A probability of 0 has a logarithm of -∞.\n",
        "3.\tThe entropy is the negative of the sum of each probability times its base-2 logarithm. For a coin toss, this is $E = -(P_H\\log_2P_H + P_T\\log_2P_T) = -(0.5\\times -1 + 0.5 \\times -1) = 1$.\n",
        "4.\tTo calculate the overall entropy when we split the examples into two groups, we must add up the entropy for each group multiplied by the probability that an example falls into the group.\n",
        "\n",
        "Consider the entropy of the walking examples if they are split according to time. The entropy of the Yes group is,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "E_{Yes} &= -(P_{Walk}\\log_2P_{Walk} + P_{No\\_Walk}\\log_2P_{No\\_Walk}) \\\\\n",
        "  &= -(2/3\\log_22/3 + 1/3\\log_21/3) \\\\\n",
        "  &= -(2/3(-0.59) + 1/3(-1.59)) \\\\\n",
        "  &= 0.92\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The entropy of the No group is zero, because all the outcomes in this group are the same.\n",
        "\n",
        "To calculate the total entropy, note that there are 6/9 examples in the Yes group with entropy $E_{Yes}=0.92$, and 3/9 examples in the No group with $E_{No}=0$. So the total entropy is $E=(6/9).92+(3/9)0=.61$. This is substantially lower than the entropy of a coin toss. This means that if we split according to Time, the decision will be substantially more certain than the outcome of a coin toss.   \n",
        "\n",
        "The code below calculates this result. If you like, you can change the numbers to see how they affect the entropy.  "
      ],
      "metadata": {
        "id": "pbO28Rko4E-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def entropy(probabilities):\n",
        "  \"\"\"\n",
        "  :param probabilities: list of probabilities of each possible outcome\n",
        "  :return: entropy of the outcome\n",
        "  \"\"\"\n",
        "  probabilities = np.array(probabilities) / np.sum(probabilities) # make sure probabilities sum to one\n",
        "  return - np.sum(probabilities * np.log2(np.clip(probabilities, 1e-12, 1))) # clip prevents divide by 0"
      ],
      "metadata": {
        "id": "BVuTwK6KhRz6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group1_negative = 2 #number of cases with time=yes, walk=no\n",
        "group1_positive = 4 #number of cases with time=yes, walk=yes\n",
        "group2_negative = 3 #number of cases with time=no, walk=no\n",
        "group2_positive = 0 #number of cases with time=no, walk=yes\n",
        "\n",
        "group1 = group1_negative + group1_positive\n",
        "group2 = group2_negative + group2_positive\n",
        "total = group1 + group2\n",
        "\n",
        "entropy_group1 = entropy([group1_negative, group1_positive])\n",
        "entropy_group2 = entropy([group2_negative, group2_positive])\n",
        "print('Entropy of Group 1: {}'.format(entropy_group1))\n",
        "print('Entropy of Group 2: {}'.format(entropy_group2))\n",
        "\n",
        "total_entropy = group1/total*entropy_group1 + group2/total*entropy_group2\n",
        "print('Total entropy: {}'.format(total_entropy))"
      ],
      "metadata": {
        "id": "YvtMGCgFmo7Z",
        "outputId": "a3fd9a5a-35d1-42c6-acd2-a7938b410fb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of Group 1: 0.9182958340544896\n",
            "Entropy of Group 2: -0.0\n",
            "Total entropy: 0.612197222702993\n"
          ]
        }
      ]
    }
  ]
}
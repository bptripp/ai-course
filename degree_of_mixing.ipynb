{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfLh2qMSFVXIL6szorC4ai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bptripp/ai-course/blob/main/degree_of_mixing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantifying how Mixed the Outcomes Are\n",
        "Different decision tree algorithms use different measures of “mixed”, including Gini impurity and information-theoretic entropy. Let's take a closer look at the latter. \n",
        "\n",
        "Entropy is the degree of uncertainty in a variable (e.g. the decision to walk or not). This kind of entropy is related to but distinct from thermodynamic “entropy”, which is the degree of disorder in a system. Information-theoretic entropy is highest when different outcomes are equally likely, and zero when the outcome is certain. A unit of entropy is called a “bit”. A fair coin toss that has not yet occurred has one bit of entropy, and its outcome has one bit of information.\n",
        "\n",
        "To calculate information entropy:\n",
        "1.\tList the probabilities of different outcomes. For example, in a coin toss, the probabilities of heads and tails are both 0.5. We could write this fact as $PH=PT=0.5$. \n",
        "2.\tCalculate the base-2 logarithm of each probability, i.e. $\\log_2P$. The base-2 logarithm of a number is the exponent of 2 that produces that number. $2-1=0.5$, so the $\\log_20.5=-1$. A probability of 1 (the highest possible) has a logarithm of 0. Lower probabilities have negative logarithms. \n",
        "3.\tThe entropy is the negative of the sum of each probability times its base-2 logarithm. For a coin toss, this is $E = -(PH\\log_2PH + PT\\log_2PT) = -(0.5 x -1 + 0.5 x -1) = 1$. \n",
        "4.\tTo calculate the overall entropy when we split the examples into two groups, we must add up the entropy for each group multiplied by the probability that an example falls into the group. If we split the Walk examples according to Time, there are 6/9 examples in the Yes group with entropy $E_{Yes}=0.92$, and 3/9 examples in the No group with $E_{No}=0$. So the total entropy is $E=(6/9).92+(3/9)0=.613$. This is substantially lower than the entropy of a coin toss. This means that if we split according to Time, the decision will be substantially more certain than the outcome of a coin toss.   \n"
      ],
      "metadata": {
        "id": "pbO28Rko4E-v"
      }
    }
  ]
}